{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27b49f6",
   "metadata": {},
   "source": [
    "**1. What is prior probability? Give an example.** \n",
    "\n",
    "- Ans : The prior knowledge or belief about the probabilities of various hypotheses in hypothesis space H is called Prior in context of Bayes’ theorem. \n",
    "\n",
    "    As an example, if we have to determine whether a particular type of tumour is malignant for a patient, the prior knowledge of such tumours becoming malignant can be used to validate our current hypothesis and is a prior probability or simply called Prior.\n",
    "\n",
    "\n",
    "\n",
    "**2. What is posterior probability? Give an example.** \n",
    "\n",
    "- Ans : The probability that a particular hypothesis holds for a data set based on the Prior is called the posterior probability or simply Posterior. In the above example, the probability of the hypothesis that the patient has a malignant tumour considering the Prior of correctness of the malignancy test is a posterior probability. In our notation, we will say that we are interested in finding out P(h|T), which means whether the hypothesis holds true given the observed training data T. This is called the posterior probability or simply Posterior in machine learning language. So, the prior probability P(h), which represents the probability of the hypothesis independent of the training data (Prior), now gets refined with the introduction of influence of the training data as P(h|T).\n",
    "\n",
    "\n",
    "\n",
    "**3. What is likelihood probability? Give an example.** \n",
    "\n",
    "- Ans : Likelihood refers to how well a sample provides support for particular values of a parameter in a model. So, liklihood is termed with respect to the data and probability is associated to a particular outcome of a result. \n",
    "    \n",
    "    When calculating the probability of some outcome, we assume the parameters in a model are trustworthy. However, when we calculate likelihood we’re trying to determine if we can trust the parameters in a model based on the sample data that we’ve observed.\n",
    "    \n",
    "    As and example we can think of tossing coins. The probability to get a head or tail is 0.5. Now, suppose a coin is tossed 100 times and 20 times we obtain head. Now, if we need to know the likelihood that the coin is fair. Now, if the coin would have been a fair coin, we would have obtained heads more tha 20 times, at least 40 - 60 times. Now given the data, we would highly suspect the fairness of the coin and hence we cannot quote the value of probability parameter as 0.5 for the above mentioned coin. \n",
    "\n",
    "\n",
    "\n",
    "**4. What is Naïve Bayes classifier? Why is it named so?** \n",
    "\n",
    "- Ans :  Naive Bayes classifier is a classification algorithm based on Baye's Theorem. This uses probabilistic approach. Given a prior probability, the posterior probability is obtained and on the basis of this class labels are assigned. \n",
    "\n",
    "    One of the most important assumption for this method is the independence and importance of all the features. Many a times, this assumption does not hold good. Because of the these assumptions, the classifier is termed as 'naive'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**5. What is optimal Bayes classifier?** \n",
    "\n",
    "- Ans : The approach in the Bayes optimal classifier is to calculate the most probable classification of each new instance on the basis of the combined predictions of all alternative hypotheses, weighted by their posterior probabilities.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**6. Write any two features of Bayesian learning methods.** \n",
    "\n",
    "- Ans : \n",
    "\n",
    "- Each observed training example can incrementally decrease or increase the estimated probability that a hypothesis is correct. \n",
    "\n",
    "- This provides a more flexible approach to learning than algorithms that completely eliminate ahypothesis if it is found to be inconsistent with any single example.\n",
    "\n",
    "- Prior knowledge can be combined with observed data to determine the final probability of a hypothesis. In Bayesian learning, prior knowledge is provided by asserting a prior probability for each candidate hypothesis, and a probability distribution over observed data for each possible hypothesis.\n",
    "\n",
    "- Bayesian methods can accommodate hypotheses that make probabilistic predictions. \n",
    "\n",
    "- New instances can be classified by combining the predictions of multiple hypotheses, weighted by their probabilities.\n",
    "\n",
    "- Even in cases where Bayesian methods prove computationally intractable,they can provide a standard of optimal decision making against which other practical methods can be measured.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**7. Define the concept of consistent learners.** \n",
    "\n",
    "- Ans : The group of learners who commit zero error over the training data and output the hypothesis are called consistent learners. If the training data is noise free and deterministic (i.e. P(D|h) = 1 if D and h are consistent and 0 otherwise) and if there is uniform prior probability distribution over H (so, P(h m ) = P(h n ) for all m, n), then every consistent learner outputs the MAP hypothesis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**8. Write any two strengths of Bayes classifier.**\n",
    "\n",
    "- Ans : Strengths - \n",
    "\n",
    "    - Simple and fast in calculation. Works good for small and large dataset. \n",
    "    - Performs well with missing and noisy data. \n",
    "\n",
    "\n",
    "\n",
    "**9. Write any two weaknesses of Bayes classifier.** \n",
    "\n",
    "- Ans : Weakness - \n",
    "\n",
    "    - The basic assumptions of equal importance and independence does not hold true always. \n",
    "    - Works good with categorical data. If the dataset contains large amount of numerical data, the reliability of the outcomes become limited. \n",
    "    \n",
    "    \n",
    "\n",
    "**10. Explain how Naïve Bayes classifier is used for** \n",
    "\n",
    "    1. Text classification\n",
    "\n",
    "    2. Spam filtering\n",
    "\n",
    "    3. Market sentiment analysis\n",
    "    \n",
    "    \n",
    "    \n",
    "- Ans : \n",
    "\n",
    "- Text Classification : Naïve Bayes classifier is among the most successful known algorithms for learning to classify text documents. It classifies the document where the probability of classifying the text is more. It uses the above algorithm to check the permutation and combination of the probability of classifying a document under a particular ‘Title’. It has various applications in document categorization, language detection, and sentiment detection, which are very useful for traditional retailers, e-retailors, and other businesses on judging the sentiments of their clients on the basis of keywords in feedback forms, social media comments, etc.\n",
    "\n",
    "\n",
    "- Spam filtering : Spam filtering is the best known use of Naïve Bayesian text classification. Presently, almost all the email providers have this as a built-in functionality, which makes use of a Naïve Bayes classifier to identify spam email on the basis of certain conditions and also the probability of classifying an email as ‘Spam’. Naïve Bayesian spam sifting has turned into a mainstream mechanism to recognize illegitimate a spam email from an honest-to-goodness email (sometimes called ‘ham’). Users can also install separate email filtering programmes. Server-side email filters such as DSPAM, Spam Assassin, Spam Bayes, and ASSP make use of Bayesian spam filtering techniques, and the functionality is sometimes embedded within the mail server software itself.\n",
    "\n",
    "\n",
    "- Market sentiment analysis : Naive Bayes is used for Market sentiment analysis, in which the Naïve Bayes classifier employs ‘single words’ and ‘word pairs’ like features and determines the sentiments of the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca49fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
