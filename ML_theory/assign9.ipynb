{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "fb995f47",
   "metadata": {},
   "source": [
    "**1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.** \n",
    "\n",
    "\n",
    "- Ans : Feature engineering is the pre-processing step of machine learning, which is used to transform raw data into features that can be used for creating a predictive model using Machine learning or statistical Modelling. Feature engineering in machine learning aims to improve the performance of models.\n",
    "\n",
    "    Feature engineering in ML contains mainly four processes: Feature Creation, Transformations, Feature Extraction, and Feature Selection.  \n",
    "\n",
    "    These processes are described as below:  \n",
    "\n",
    "    - Feature Creation: Feature creation is finding the most useful variables to be used in a predictive model. The process is subjective, and it requires human creativity and intervention. The new features are created by mixing existing features using addition, subtraction, and ration, and these new features have great flexibility.  \n",
    "    - Transformations: The transformation step of feature engineering involves adjusting the predictor variable to improve the accuracy and performance of the model. For example, it ensures that the model is flexible to take input of the variety of data; it ensures that all the variables are on the same scale, making the model easier to understand. It improves the model's accuracy and ensures that all the features are within the acceptable range to avoid any computational error.  \n",
    "    - Feature Extraction: Feature extraction is an automated feature engineering process that generates new variables by extracting them from the raw data. The main aim of this step is to reduce the volume of data so that it can be easily used and managed for data modelling. Feature extraction methods include cluster analysis, text analytics, edge detection algorithms, and principal components analysis (PCA).  \n",
    "    - Feature Selection: While developing the machine learning model, only a few variables in the dataset are useful for building the model, and the rest features are either redundant or irrelevant. If we input the dataset with all these redundant and irrelevant features, it may negatively impact and reduce the overall performance and accuracy of the model. Hence it is very important to identify and select the most appropriate features from the data and remove the irrelevant or less important features, which is done with the help of feature selection in machine learning. \"Feature selection is a way of selecting the subset of the most relevant features from the original features set by removing the redundant, irrelevant, or noisy features.\"  \n",
    "    \n",
    "    \n",
    "    \n",
    "    **Steps in Feature Engineering**\n",
    "\n",
    "    The steps of feature engineering may vary as per different data scientists and ML engineers. However, there are some common steps that are involved in most machine learning algorithms, and these steps are as follows:\n",
    "\n",
    "    - Data Preparation: The first step is data preparation. In this step, raw data acquired from different resources are prepared to make it in a suitable format so that it can be used in the ML model. The data preparation may contain cleaning of data, delivery, data augmentation, fusion, ingestion, or loading.\n",
    "    \n",
    "    - Exploratory Analysis: Exploratory analysis or Exploratory data analysis (EDA) is an important step of features engineering, which is mainly used by data scientists. This step involves analysis, investing data set, and summarization of the main characteristics of data. Different data visualization techniques are used to better understand the manipulation of data sources, to find the most appropriate statistical technique for data analysis, and to select the best features for the data.\n",
    "    \n",
    "    - Benchmark: Benchmarking is a process of setting a standard baseline for accuracy to compare all the variables from this baseline. The benchmarking process is used to improve the predictability of the model and reduce the error rate.\n",
    "    \n",
    "    \n",
    "   **Some of the popular feature engineering techniques include:**\n",
    "\n",
    " 1. Imputation : Feature engineering deals with inappropriate data, missing values, human interruption, general errors, insufficient data sources, etc. Missing values within the dataset highly affect the performance of the algorithm, and to deal with them \"Imputation\" technique is used. Imputation is responsible for handling irregularities within the dataset. \n",
    "      - For example, removing the missing values from the complete row or complete column by a huge percentage of missing values. But at the same time, to maintain the data size, it is required to impute the missing data, which can be done as:\n",
    "    - For numerical data imputation, a default value can be imputed in a column, and missing values can be filled with means or medians of the columns.\n",
    "    - For categorical data imputation, missing values can be interchanged with the maximum occurred value in a column.\n",
    "\n",
    "\n",
    "\n",
    "2. Handling Outliers : Outliers are the deviated values or data points that are observed too away from other data points in such a way that they badly affect the performance of the model. Outliers can be handled with this feature engineering technique. This technique first identifies the outliers and then remove them out.\n",
    "\n",
    "    Standard deviation can be used to identify the outliers. For example, each value within a space has a definite to an average distance, but if a value is greater distant than a certain value, it can be considered as an outlier. Z-score can also be used to detect outliers.\n",
    "\n",
    "\n",
    "\n",
    "3. Log transform :  Logarithm transformation or log transform is one of the commonly used mathematical techniques in machine learning. Log transform helps in handling the skewed data, and it makes the distribution more approximate to normal after transformation. It also reduces the effects of outliers on the data, as because of the normalization of magnitude differences, a model becomes much robust.\n",
    "\n",
    "    Note: Log transformation is only applicable for the positive values; else, it will give an error. To avoid this, we can add 1 to the data before transformation, which ensures transformation to be positive.\n",
    "\n",
    "\n",
    "\n",
    "4. Binning : In machine learning, overfitting is one of the main issues that degrade the performance of the model and which occurs due to a greater number of parameters and noisy data. However, one of the popular techniques of feature engineering, \"binning\", can be used to normalize the noisy data. This process involves segmenting different features into bins.\n",
    "\n",
    "\n",
    "\n",
    "5. Feature Split : As the name suggests, feature split is the process of splitting features intimately into two or more parts and performing to make new features. This technique helps the algorithms to better understand and learn the patterns in the dataset.\n",
    "\n",
    "    The feature splitting process enables the new features to be clustered and binned, which results in extracting useful information and improving the performance of the data models.\n",
    "\n",
    "\n",
    "6. One hot encoding : One hot encoding is the popular encoding technique in machine learning. It is a technique that converts the categorical data in a form so that they can be easily understood by machine learning algorithms and hence can make a good prediction. It enables group the of categorical data without losing any information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?** \n",
    "\n",
    "- Ans : Feature selection is the process where the features are automatically or manually selected that contribute the most to the prediction variable or output. Feature selection is performed by either including the important features or excluding the irrelevant features in the dataset without changing them.\n",
    "\n",
    "    The aim to Feature selection is to reduce the number of input variables by eliminating redundant or irrelevant features. It then narrows the set of features to those most relevant to the machine learning model. A feature selection objective in machine learning identifies the most helpful group of features that can be used to build useful models of the phenomena being studied.\n",
    "   \n",
    "   In the machine learning process, feature selection is used to make the process more accurate. It also increases the prediction power of the algorithms by selecting the most critical variables and eliminating the redundant and irrelevant ones. This is why feature selection is important.\n",
    "\n",
    "    Three key benefits of feature selection are:\n",
    "\n",
    "    - Decreases over-fitting : fewer redundant data means fewer chances of making decisions based on noise.\n",
    "    - Improves Accuracy : Less misleading data means better modeling accuracy.\n",
    "    - Reduces Training Time : Less data means quicker algorithms.\n",
    "    \n",
    "\n",
    "\n",
    "  Various methods of feature selections are : \n",
    "    \n",
    " - Supervised Feature Selection Methods\n",
    "\n",
    "    Supervised feature selection methods are classified into four types, based on the interaction with the learning model, such as the Filter, Wrapper, Hybrid, and Embedded Methods.\n",
    "\n",
    "    - Filter Methodology : In the Filter method, features are selected based on statistical measures. It is independent of the learning algorithm and requires less computational time. Information gain, chi-square test, Fisher score, correlation coefficient, and variance threshold are some of the statistical measures used to understand the importance of the features.  \n",
    "        The Filter methodology uses the selected metric to identify irrelevant attributes and also filter out redundant columns from the models. It gives the option of isolating selected measures that enrich a model. The columns are ranked following the calculation of the feature scores.    \n",
    "\n",
    "    - Wrapper Methodology : The Wrapper methodology considers the selection of feature sets as a search problem, where different combinations are prepared, evaluated, and compared to other combinations. A predictive model is used to evaluate a combination of features and assign model performance scores.      \n",
    "        The performance of the Wrapper method depends on the classifier. The best subset of features is selected based on the results of the classifier.    \n",
    "        \n",
    "   - Hybrid Methodology : The process of creating hybrid feature selection methods depends on what you choose to combine. The main priority is to select the methods you’re going to use, then follow their processes. The idea here is to use these ranking methods to generate a feature ranking list in the first step, then use the top k features from this list to perform wrapper methods. With that, we can reduce the feature space of our dataset using these filter-based rangers in order to improve the time complexity of the wrapper methods.\n",
    "  \n",
    "   - Embedded Methodology : In embedded techniques, the feature selection algorithm is integrated as part of the learning algorithm. The most typical embedded technique is the decision tree algorithm. Decision tree algorithms select a feature in each recursive step of the tree growth process and divide the sample set into smaller subsets.\n",
    "\n",
    " \n",
    "- Unsupervised Feature Selection Methods\n",
    "\n",
    "     Due to the scarcity of readily available labels, unsupervised feature selection (UFS) methods are widely adopted in the analysis of high-dimensional data. However, most of the existing UFS methods primarily focus on the significance of features in maintaining the data structure while ignoring the redundancy among features. Moreover, the determination of the proper number of features is another challenge.\n",
    "\n",
    "    Unsupervised feature selection methods are classified into four types, based on the interaction with the learning model, such as the Filter, Wrapper, and Hybrid methods.\n",
    "\n",
    "    - Filter Methodology : Unsupervised feature selection methods based on the filter approach can be categorized as univariate and multivariate. Univariate methods, aka ranking-based unsupervised feature selection methods, use certain criteria to evaluate each feature to get an ordered ranking list of features, where the final feature subset is selected according to this order. Such methods can effectively identify and remove irrelevant features, but they are unable to remove redundant ones since they do not take into account possible dependencies among features. On the other hand, multivariate filter methods evaluate the relevance of the features jointly rather than individually. Multivariate methods can handle redundant and irrelevant features. Thus, in many cases, the accuracy reached by learning algorithms using the subset of features selected by multivariate methods is better than the one achieved by using univariate methods.\n",
    "\n",
    "  - Wrapper Methodology : Unsupervised feature selection methods based on the wrapper approach can be divided into three broad categories according to the feature search strategy: sequential, bio-inspired, and iterative. In sequential methodology, features are added or removed sequentially. Methods based on sequential search are easy to implement and fast.\n",
    "\n",
    "    On the other hand, a bio-inspired methodology tries to incorporate randomness into the search process, aiming to escape local optima.\n",
    "\n",
    "    Iterative methods address the unsupervised feature selection problem by casting it as an estimation problem and thus avoiding a combinatorial search.\n",
    "\n",
    "    Wrapper methods evaluate feature subsets using the results of a specific clustering algorithm. Methods developed under this approach are characterized by finding feature subsets that contribute to improving the quality of the results of the clustering algorithm used for the selection. However, the main disadvantage of wrapper methods is that they usually have a high computational cost, and they are limited to be used in conjunction with a particular clustering algorithm.\n",
    "\n",
    "  - Hybrid Methodology : Hybrid methods try to exploit the qualities of both approaches, filter and wrapper, trying to have a good compromise between efficiency (computational effort) and effectiveness (quality in the associated objective task when using the selected features).\n",
    "\n",
    "    In order to take advantage of the filter and wrapper approaches, hybrid methods include a filter stage where the features are ranked or selected by applying a measure based on the intrinsic properties of the data. While, in a wrapper stage, certain feature subsets are evaluated for finding the best one through a specific clustering algorithm. We can distinguish two types of hybrid methods: methods based on ranking and methods not based on the ranking of features.\n",
    "\n",
    "\n",
    "\n",
    "**3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?** \n",
    "\n",
    "- Ans :  \n",
    "\n",
    "- Filter Method selects features on the basis of statistics measures. This method does not depend on the learning algorithm and chooses the features as a pre-processing step. The filter method filters out the irrelevant feature and redundant columns from the model by using different metrics through ranking. Eg: Information Gain, Chi-Square Test, Fisher’s Score, etc. \n",
    "\n",
    "    - Advantages: \n",
    "        - Computationally cheaper as compared to wrapper or embedded method. \n",
    "        - Fastest running time.\n",
    "        - Lower risk of overfitting \n",
    "        - Ability of good generalization\n",
    "        - Easy scaling for high dimensional datset.\n",
    "    - Disadvantages : \n",
    "        - No interaction with classification model for feature selection.\n",
    "        - Mostly ignores feature dependencies and considers each feature seperately in case of univariate techniques, which may lead to low computational performance as compared to the other techniques of feature selection. \n",
    "        \n",
    "        \n",
    "\n",
    "- Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "   The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy.\n",
    "\n",
    "    Most commonly used techniques under wrapper methods are:\n",
    "\n",
    "    1.Forward selection: In forward selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature with the minimum p-value. Now fit a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again select the feature with the minimum p-value. Now fit a model with three features by trying combinations of two previously selected features with other remaining features. Repeat this process until we have a set of selected features with a p-value of individual features less than the significance level.\n",
    "\n",
    "    2.Backward elimination: In backward elimination, we start with the full model (including all the independent variables) and then remove the insignificant feature with the highest p-value(> significance level). This process repeats again and again until we have the final set of significant features\n",
    "\n",
    "    3.Bi-directional elimination(Stepwise Selection): It is similar to forward selection but the difference is while adding a new feature it also checks the significance of already added features and if it finds any of the already selected features insignificant then it simply removes that particular feature through backward elimination. Hence, It is a combination of forward selection and backward elimination.\n",
    "\n",
    "  - Advantages: \n",
    "    - Interacts with the classifier for feature selection. \n",
    "    - Consider feature dependencies. \n",
    "    - Measure the “usefulness” of features based on the classifier performance.\n",
    "  - Disadvantages : \n",
    "    - The increasing overfitting risk when the number of observations is insufficient.\n",
    "    - The significant computation time when the number of variables is large\n",
    "\n",
    "\n",
    "**4.**\n",
    "    \n",
    "    i. Describe the overall feature selection process.\n",
    "\n",
    "    ii. Explain the key underlying principle of feature extraction using an example. What are the mostwidely used function extraction algorithms?\n",
    "    \n",
    "- Ans : \n",
    "\n",
    "  i) Overall Feature selection process : A typical feature selection process consists of four steps:\n",
    "    1. generation of possible subsets  \n",
    "    2. subset evaluation  \n",
    "    3. stop searching based on some stopping criterion  \n",
    "    4. validation of the result    \n",
    "  \n",
    "  Subset generation, which is the first step of any feature selection algorithm, is a search procedure which ideally should produce all possible candidate subsets. However, for an n-dimensional data set, 2 n subsets can be generated. So, as the value of ‘n’ becomes high, finding an optimal subset from all the 2 n candidate subsets becomes intractable. For that reason,different approximate search strategies are employed to find candidate subsets for evaluation. On one hand, the search may start with an empty set and keep adding features. This search strategy is termed as a sequential forward selection. On the other hand, a search may start with a full set and successively remove features. This strategy is termed as sequential backward elimination. In certain cases, search start with both ends and add and remove features simultaneously. This strategy is termed as a bi-directional selection. Each candidate subset is then evaluated and compared with the previous best performing subset based on certain evaluation criterion. If the new subset performs better, it replaces the previous one. This cycle of subset generation and evaluation continues till a pre-defined stopping criterion is fulfilled. Some commonly used stopping criteria are: \n",
    "  1. the search completes  \n",
    "  2. some given bound (e.g. a specified number of iterations) is reached  \n",
    "  3. subsequent addition (or deletion) of the feature is not producing a better subset  \n",
    "  4. a sufficiently good subset (e.g. a subset having better classification accuracy than the existing benchmark) is selected  \n",
    "    \n",
    "  Then the selected best subset is validated either against prior benchmarks or by experiments using real-life or synthetic but authentic data sets. In case of supervised learning, the accuracy of the learning model may be the performance parameter considered for validation. The accuracy of the model using the subset derived is compared against the model accuracy of the subset derived using some other benchmark algorithm. In case of unsupervised, the cluster quality may be the parameter for validation.  \n",
    "      \n",
    "  ii)  Underlying principle of feature extraction using an example. What are the mostwidely used function extraction algorithms?   \n",
    "  In feature extraction, new features are created from a combination of original features. Some of the commonly used operators for combining the original features include  \n",
    "  1. For Boolean features: Conjunctions, Disjunctions, Negation, etc.  \n",
    "  2. For nominal features: Cartesian product, M of N, etc.  \n",
    "  3. For numerical features: Min, Max, Addition, Subtraction, Multiplication, Division, Average, Equivalence, Inequality, etc.   \n",
    "    \n",
    "    For feature extraction mostly PCA, SVD, LDA are very commonly used. \n",
    "    \n",
    "    \n",
    "  \n",
    "**5. Describe the feature engineering process in the sense of a text categorization issue.** \n",
    "\n",
    "- Ans : Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data. Example: Text cleaning steps vary according to the type of data and the required task. Generally, the string is converted to lowercase and punctuation is removed before text gets tokenized. Tokenization is the process of splitting a string into a list of strings (or “tokens”). We can create a list of generic stop words for the English vocabulary with NLTK (the Natural Language Toolkit), which is a suite of libraries and programs for symbolic and statistical natural language processing. Then we can remove these stop words. We need to be very careful with stop words because if you remove the wrong token you may lose important information. For example, the word “will” was removed and we lost the information that the person is Will Smith. With this in mind, it can be useful to do some manual modification to the raw text before removing stop words (for example, replacing “Will Smith” with “Will_Smith”). Stemming and Lemmatization both generate the root form of words. The difference is that stem might not be an actual word whereas lemma is an actual language word (also stemming is usually faster). After this, the text data must be represented as vectors and fed to machine learning models. This step can be performed using different approaches, like - Bag of Words (BOW), TF-IDF, n-gram. All these are provided by NLTK library. \n",
    "\n",
    "\n",
    "\n",
    "**6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine.** \n",
    "\n",
    "- Ans :  Cosine similarity is one of the metric to measure the text-similarity between two documents irrespective of their size in Natural language Processing. A word is represented into a vector form. The text documents are represented in n-dimensional vector space.\n",
    "\n",
    "    Mathematically, Cosine similarity metric measures the cosine of the angle between two n-dimensional vectors projected in a multi-dimensional space. The Cosine similarity of two documents will range from 0 to 1. If the Cosine similarity score is 1, it means two vectors have the same orientation. The value closer to 0 indicates that the two documents have less similarity.\n",
    "    \n",
    "    The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "   The cosine similarity between document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1) :  \n",
    " \n",
    "$\\theta$ = $\\frac{4+3+0+0+6+6+3+0+1}{\\sqrt{4+9+4+4+9+9+1} \\sqrt{4+1+9+4+1+9+1}}$ = $\\frac{23}{\\sqrt{40} \\sqrt{29}}$ = 0.675\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**7.**\n",
    "\n",
    "    i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "    ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "\n",
    "- Ans : \n",
    "    i) Hamming distance is a metric for comparing two binary data strings. While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different.In the above case 10001011 and 11001111 , hamming distance is 2.  \n",
    "    \n",
    "    ii)  For details please go through : https://online.stat.psu.edu/stat508/lesson/1b/1b.2/1b.2.1   \n",
    "    \n",
    "    For the given case : p = (11000111), q = (11001011)\n",
    "    \n",
    "                q = 1 | q = 0\n",
    "  \n",
    "         p = 1|   4   |   1\n",
    "  \n",
    "         p = 0|   1   |   2 \n",
    "         \n",
    "        \n",
    "        SMC = (4+2)/(4+1+1+1) = 6/8 = 0.75 \n",
    "        Jaccard index = 4/(4+1+1) = 0.66\n",
    "\n",
    "\n",
    "\n",
    "**8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**9. Make a few quick notes on:**\n",
    "\n",
    "    1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "    2. Use of vectors\n",
    "\n",
    "    3. Embedded technique\n",
    "\n",
    "\n",
    "\n",
    "- Ans : \n",
    "     1. PCA : Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. PCA transforms data linearly into new properties that are not correlated with each other. For ML, positioning PCA as feature extraction may allow us to explore its potential better than dimension reduction. PCA works based on a process called eigenvalue decomposition of a covariance matrix of a data set. Below are the steps to be followed:  \n",
    "        a. First, calculate the covariance matrix of a data set. \n",
    "        b. Then, calculate the eigenvalues of the covariance matrix.  \n",
    "        c. The eigenvector having highest eigenvalue represents the direction in which there is the highest variance. So this will help in identifying thefirst principal component.   \n",
    "        d. The eigenvector having the next highest eigenvalue represents the direction in which data has the highest remaining variance and also orthogonal to the first direction. So this helps in identifying the second principal component.  \n",
    "        e. Like this, identify the top ‘k’ eigenvectors having top ‘k’ eigenvalues so as to get the ‘k’ principal components.  \n",
    "\n",
    "  2. Use of vectors : A vector is a quantity having both magnitude and direction and hence can determine the position of a point relative to another point in the Euclidean space (i.e. a two or three or ‘n’ dimensional space). A vector space is a set of vectors. Vector spaces have a property that they can be represented as a linear combination of a smaller set of vectors, called basis vectors. So, any vector ‘v’ in a vector space can be represented as: $v$ = $\\sum_i^n$ $a_i u_i$, where, $a_i$ represents ‘n’ scalars and $u_i$ represents the basis vectors. Basis vectors are orthogonal to each other.\n",
    "     Orthogonality of vectors in n-dimensional vector space can be thought of an extension of the vectors being perpendicular in a two-dimensional vector space. Two orthogonal vectors are completely unrelated or independent of each other. So the transformation of a set of vectors to the corresponding set ofbasis vectors such that each vector in the original set can be expressed as a linear combination of basis vectors helps in decomposing the vectors to a number of independent components.  \n",
    "     Now, let’s extend this notion to the feature space of a data set. The feature vector can be transformed to a vector space of the basis vectors which are termed as principal components. These principal components, just like the basis vectors, are orthogonal to each other. So a set of feature vectors which may have similarity with each other is transformed to a set of principal components which are completely unrelated. However, the principal components capture the variability of the original feature space. Also, the number of principal component derived, much like the basis vectors, is much smaller than the original set of features.\n",
    "     \n",
    "  3. Embedded technique : Embedded approach for feature selection is quite similar to wrapper approach as it also uses and inductive algorithm to evaluate the generated feature subsets. However, the difference is it performs feature selection and classification simultaneously.\n",
    "\n",
    "\n",
    "\n",
    "**10. Make a comparison between:** \n",
    "\n",
    "    1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "    2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "    3. SMC vs. Jaccard coefficient\n",
    "    \n",
    "    \n",
    "    \n",
    "- Ans : \n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection  \n",
    "\n",
    "    In machine learning, there are two main methods for feature selection: forward selection and backward elimination. Both methods have pros and cons, and which one you use will ultimately depend on your specific data and goals.\n",
    "\n",
    "    Forward selection is a greedy algorithm that starts with an empty set of features and adds features one by one until the model performance reaches a peak. This method is simple and easy to implement but can be computationally expensive and may not find the optimal set of features.\n",
    "\n",
    "    Backward elimination is a more systematic approach that starts with a complete set of features and removes features one by one until the model performance reaches a peak. This method is more computationally efficient but may not find the optimal set of features either.\n",
    "\n",
    "\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper  \n",
    "\n",
    "   Filter Methodology : In the Filter method, features are selected based on statistical measures. It is independent of the learning algorithm and requires less computational time. Information gain, chi-square test, Fisher score, correlation coefficient, and variance threshold are some of the statistical measures used to understand the importance of the features.  \n",
    "   The Filter methodology uses the selected metric to identify irrelevant attributes and also filter out redundant columns from the models. It gives the option of isolating selected measures that enrich a model. The columns are ranked following the calculation of the feature scores.    \n",
    "\n",
    "   Wrapper Methodology : The Wrapper methodology considers the selection of feature sets as a search problem, where different combinations are prepared, evaluated, and compared to other combinations. A predictive model is used to evaluate a combination of features and assign model performance scores.      \n",
    "   The performance of the Wrapper method depends on the classifier. The best subset of features is selected based on the results of the classifier. This methods takes more computational time and more prone to overfitting. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. SMC vs. Jaccard coefficient :   \n",
    "\n",
    "   Jaccard index/coefficient is used as a measure of similarity between two features. The Jaccard distance, a measure of dissimilarity between two features, is complementary of Jaccard index. Let’s consider two features F1 and F2 having values (0, 1, 1, 0, 1, 0, 1, 0) and (1, 1, 0, 0, 1, 0, 0, 0). Jaccard coefficient of F 1 and F 2 , J = 2/(1+2+2) = 0.4.  \n",
    "   \n",
    "   Simple matching coefficient (SMC) is almost same as Jaccard coeficient except the fact that it includes a number of cases where both the features have a value of 0. SMC for the above example = (2+3)/(1+3+2+2) = 0.625. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723135b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
