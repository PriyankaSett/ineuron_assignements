{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c8d5e78",
   "metadata": {},
   "source": [
    "##  \n",
    "\n",
    "\n",
    "**1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?** \n",
    "\n",
    "- Ans : Initializing all weights to the same value, even if it's chosen using He initialization, might not be the best approach. He initialization involves randomizing the weights to prevent symmetrical learning patterns and the vanishing/exploding gradients problem during training. However, setting all weights to the same value, even if randomly chosen, might limit the network's ability to learn diverse features.\n",
    "\n",
    "  He initialization suggests scaling the initialization according to the number of input units to that layer, which helps in maintaining variance during forward and backward propagation. When all weights are set to the same value, the neural network might face challenges in breaking symmetry and learning distinct representations, leading to reduced performance.\n",
    "\n",
    "  It's recommended to apply He initialization with random values drawn from a distribution like a Gaussian or uniform distribution to break symmetries and promote efficient learning in neural networks. This way, each neuron can start with different initial weights, aiding in the learning of diverse features and enhancing the network's capacity to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b948803",
   "metadata": {},
   "source": [
    "**2. Is it okay to initialize the bias terms to 0?** \n",
    "\n",
    "- Ans : Initializing bias terms to zero is a common practice in neural network initialization. Unlike weights, biases are not symmetrically tied to the network's learning dynamics. Setting biases to zero initially is acceptable and often works well.\n",
    "\n",
    "  Biases serve to shift the activation function of each neuron, allowing them to learn the best fit for the data by controlling the \"starting point\" of the activation. Initializing biases to zero initially doesn't hinder the learning process since the network can adjust these values during training.\n",
    "\n",
    "  However, in certain cases, initializing biases to non-zero values can have benefits. For instance, in cases where you know or suspect that your data is not centered at zero, initializing biases to a value that represents this data distribution might help the network converge faster. But this should be done cautiously, ensuring that the initialization doesnâ€™t introduce unnecessary biases that might slow down learning or cause biases toward specific classes or features in the data.\n",
    "\n",
    "  In general, initializing biases to zero is a reasonable default strategy, but there could be scenarios where non-zero initialization might offer advantages depending on the specifics of the problem and the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343428e",
   "metadata": {},
   "source": [
    "\n",
    "**3. Name three advantages of the ELU activation function over ReLU.** \n",
    "\n",
    "- Ans : Advantages of ELU over ReLU: \n",
    "    - ELU successfully prevent the 'Dying Relu' problem. \n",
    "    \n",
    "    - ELU has a range extending to negative values, allowing neurons to capture negative input information. This can be advantageous in tasks where both positive and negative information is essential for learning representations effectively.\n",
    "    \n",
    "    - ELU is smooth and differentiable everywhere, including at zero, which avoids potential issues related to non-differentiability at certain points. This property can sometimes lead to smoother optimization surfaces, facilitating faster convergence during training compared to ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9e45c",
   "metadata": {},
   "source": [
    "**4. In which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?** \n",
    "\n",
    "- Ans : \n",
    "    - ELU - This activation function is used for neural networks. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity.\n",
    "    \n",
    "    - Leaky ReLU - This is used in artificial neural networks to introduce nonlinearity among the outputs between layers of a neural network. This activation function was created to solve the dying ReLU problem using the standard ReLU function that makes the neural network die during training.\n",
    "    \n",
    "    - ReLU - This function is mostly used in hidden layers inspite of having some issues like, 'Dying ReLU' and also it is not differentiable at 0 value. Still it is most widely used activation function as the computation is fast and also as this function  has no upper bound, quite a times helps in reducing some issues with gradient descent. \n",
    "    \n",
    "    - tanh - This is used in hidden layers so that the output remains between -1 and +1 centered around 0. This helps in faster convergence.  \n",
    "    \n",
    "    - logistic - This is used at output layer for binary classification. \n",
    "    \n",
    "    - softmax - This is used at the output layer for multiclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71faaf80",
   "metadata": {},
   "source": [
    "\n",
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?** \n",
    "\n",
    "- Ans : The momentum hyperparameter $\\beta$ in Momentum Optimizer is used to simulate friction and prevent the momentum from growing too large. The value of $\\beta$ = 0 means high friction and 1 means no friction. So, if $\\beta$ is set close to 1, the optimization process will be faster but the algorithm may oscillate around the optimal result. This will lead to slower convergence and sometimes overfitting too.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8074ae",
   "metadata": {},
   "source": [
    "\n",
    "**6. Name three ways you can produce a sparse model.** \n",
    "\n",
    "- Ans : \n",
    "    - Use a weight regularization technique such as L1 regularization, which encourages the model to set the weights for less important features to zero.\n",
    "\n",
    "    - Use a pruning technique such as magnitude pruning, which removes weights with small magnitudes from the model.\n",
    "\n",
    "    - Use a model architecture that produces sparse models such as a convolutional neural network, which has fewer parameters due to the shared weights across different convolutional layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9580b1c",
   "metadata": {},
   "source": [
    "\n",
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?** \n",
    "\n",
    "- Ans : Yes, dropout does slow down the training. Howerver it has no impact on th inference since it only turn on during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75969800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
